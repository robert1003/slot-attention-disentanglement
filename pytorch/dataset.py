import os
import random
import json
import numpy as np
from PIL import Image
import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataloader import default_collate
from torchvision.datasets import VisionDataset


class PARTNET(Dataset):
    def __init__(self, split='train'):
        super(PARTNET, self).__init__()

        assert split in ['train', 'val', 'test']
        self.split = split
        self.root_dir = your_path     
        self.files = os.listdir(self.root_dir)
        self.img_transform = transforms.Compose([
               transforms.ToTensor()])

    def __getitem__(self, index):
        path = self.files[index]
        image = Image.open(os.path.join(self.root_dir, path, "0.png")).convert("RGB")
        image = image.resize((128 , 128))
        image = self.img_transform(image)
        sample = {'image': image}

        return sample
            
    
    def __len__(self):
        return len(self.files)

class CLEVR(Dataset):
    def __init__(self, path, split='train', rescale=False):
        super(CLEVR, self).__init__()

        assert split in ['train', 'val', 'test']
        self.split = split
        self.root_dir = os.path.join(path, split)
        self.files = os.listdir(self.root_dir)
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])

    def __getitem__(self, index):
        path = self.files[index]
        image = Image.open(os.path.join(self.root_dir, path)).convert("RGB")
        image = image.resize((128 , 128))
        image = self.img_transform(image)
        sample = {'image': image}

        return sample

    def __len__(self):
        return len(self.files)
    
class MultiDSprites(Dataset):
    '''
    Deprecated. This is for dataset randomly generated by genesis code
    '''
    def __init__(self, path='./data/multi_dsprites/processed', split='train', unique=True, num_slots=6, rescale=False):
        super(MultiDSprites, self).__init__()

        assert split in ['train', 'val', 'test']
        file_split = {'train': 'training', 'val': 'validation', 'test': 'test'}[split]
        self.split = split
        self.unique = unique
        self.data = np.load(os.path.join(path,
            file_split + '_images_rand4_' + ('unique' if unique else '') + '.npy'))
        self.mask = np.load(os.path.join(path,
            file_split + '_masks_rand4_' + ('unique' if unique else '') + '.npy'))
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])
        self.num_slots = num_slots

    def __getitem__(self, index):
        image = (self.data[index]*255).astype(np.uint8)
        mask = self.mask[index]
        # Resize mask
        mask = np.array(Image.fromarray(mask.squeeze(-1).astype(np.uint8), mode='L').resize((128,128), resample = Image.Resampling.NEAREST))
        # Convert to onehot
        onehot_masks = (np.arange(1,mask.max()+1) == mask[...,None]).astype(int)
        # pad zeros
        zeros_array = np.zeros((onehot_masks.shape[0],onehot_masks.shape[1], max(0,self.num_slots - onehot_masks.shape[2])))
        onehot_masks = np.concatenate((onehot_masks, zeros_array), axis=2)
        image = Image.fromarray(image)
        image = image.resize((128 , 128))
        image = self.img_transform(image)
        sample = {'image': image, 'mask': onehot_masks}

        return sample

    def __len__(self):
        return len(self.data)

class MultiDSpritesGrayBackground(Dataset):
    '''
    object num is 2-5
    '''
    def __init__(self, path='./data/multi_dsprites/processed', rescale=False):
        super(MultiDSpritesGrayBackground, self).__init__()
        self.root_dir = path
        self.files = [i for i in os.listdir(self.root_dir) if 'image' in i]
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])

    def __getitem__(self, index):
        image_path = self.files[index]
        image = Image.fromarray(np.load(os.path.join(self.root_dir, image_path)))
        image = self.img_transform(image)

        mask_path = image_path.replace("image", "mask")
        mask = np.load(os.path.join(self.root_dir, mask_path))

        # Convert mask to 0-1 range
        mask = ((mask - mask.min()) / (mask.max() - mask.min())).astype(np.uint8)
        mask = torch.from_numpy(mask)

        # Remove background mask (always the first mask), background labels not included in ARI
        mask[0] = torch.zeros_like(mask[1])

        # Convert mask to format expected by ARI calc (H * W, # slots)
        mask = torch.permute(torch.flatten(mask.squeeze(-1), start_dim=1, end_dim=2), (1, 0))
        sample = {'image': image, 'mask': mask}
        return sample

    def __len__(self):
        return len(self.files)

class MultiDSpritesColorBackground(Dataset):
    '''
    object num is 1-4
    '''
    def __init__(self, path='./data/multi_dsprites/processed', rescale=False):
        super(MultiDSpritesColorBackground, self).__init__()
        self.root_dir = path
        self.files = [i for i in os.listdir(self.root_dir) if 'image' in i]
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])

    def __getitem__(self, index):
        image_path = self.files[index]
        image = Image.fromarray(np.load(os.path.join(self.root_dir, image_path)))
        image = self.img_transform(image)

        mask_path = image_path.replace("image", "mask")
        mask = np.load(os.path.join(self.root_dir, mask_path))

        # Convert mask to 0-1 range
        mask = ((mask - mask.min()) / (mask.max() - mask.min())).astype(np.uint8)
        mask = torch.from_numpy(mask)

        # Remove background mask (always the first mask), background labels not included in ARI
        mask[0] = torch.zeros_like(mask[1])

        # Convert mask to format expected by ARI calc (H * W, # slots)
        mask = torch.permute(torch.flatten(mask.squeeze(-1), start_dim=1, end_dim=2), (1, 0))
        sample = {'image': image, 'mask': mask}
        return sample

    def __len__(self):
        return len(self.files)



class COCO2017(VisionDataset):
    """
    A dataset for generating ViT embeddings from the COCO 2017 dataset. Roughly
    based on torchvision.datasets.CocoDetection, with adaptations to give user
    access to filenames and pre-processed target masks. 

    Dataset source: 
    `wget http://images.cocodataset.org/zips/val2017.zip`
    `wget http://images.cocodataset.org/zips/train2017.zip`
    `wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip`

    Val size: 778 MB, 5k images
    Train size: 18 GB, ~118k images
    """
    def __init__(self, path='./data/coco', split='val', resolution=(224,224)):
        # Largely taken from torchvision.datasets.CocoDetection 
        assert split in ['train', 'val']
        root = os.path.join(path, f"{split}2017")
        annFile = os.path.join(path, f"annotations/instances_{split}2017.json")

        super().__init__(root, None, None, None)
        from pycocotools.coco import COCO
        self.coco = COCO(annFile)
        self.ids = list(sorted(self.coco.imgs.keys()))
        self.resolution = resolution
        
        # Resize to (224,224) ignoring aspect ratio (DINOSAUR pg35/36, COCO 2017 object segmentation)
        self.img_transform = transforms.Compose([transforms.Resize(size=resolution), transforms.ToTensor()])

    def __getitem__(self, index):
        id = self.ids[index]
        image, name = self._load_image(id)
        target = self._load_target(id)
        image = self.img_transform(image)

        # Target is a flag for whether this example contains a valid mask
        sample = {'image': image, 'src' : name, 'target': len([ann for ann in target if ann['iscrowd'] == 0]) > 0}
        return sample

    def _load_image(self, id):
        path = self.coco.loadImgs(id)[0]["file_name"]
        return Image.open(os.path.join(self.root, path)).convert("RGB"), path

    def _load_target(self, id):
        return self.coco.loadAnns(self.coco.getAnnIds(id))

    def __len__(self):
        return len(self.ids)


class COCO2017Embeddings(COCO2017):
    """
    A dataset for handling pre-generated ViT embeddings of the COCO 2017 dataset.
    """
    def __init__(self, data_path='./data/coco', embed_path='./data/coco/embedding', split='val', resolution=(224, 224)):
        super().__init__(data_path, split, resolution)
        self.embed_path = embed_path
        self.ids = [int(i.split('.')[0]) for i in os.listdir(self.embed_path) if '.npy' in i]
        self.mask_transform = transforms.Resize(size=resolution)
        self.max_obj_per_image = 90     # max number of labeled objects in any test/validation image

    def __getitem__(self, index):
        id = self.ids[index]
        image, _ = self._load_image(id)
        target = self._load_target(id)
        embed = self._load_embedding(id)
        image = self.img_transform(image)

        # Convert annotations to masks and filter out crowd instance annotations (DINOSAUR pg36) 
        mask_list = [torch.from_numpy(self.coco.annToMask(ann)).unsqueeze(0) for ann in target if ann['iscrowd'] == 0]
        mask_list = [self.mask_transform(msk) for msk in mask_list]
        masks = torch.concat(mask_list, dim=0)
        
        # Remove overlaps between instances (DINOSAUR pg36)
        mask_sum = torch.sum(masks, dim=0)
        masks = torch.where(mask_sum > 1, 0, masks)
        # plt.imshow( mask_sum )
        # plt.imshow( torch.sum(masks, dim=0) )

        # Pad out masks to max number of objects 
        mask_shape = masks.shape
        num_pad = self.max_obj_per_image - mask_shape[0]
        assert num_pad >= 0, f"Found more object labels than max_obj_per_image: {mask_shape[0]}"
        if num_pad > 0:
            padding = torch.zeros((num_pad, mask_shape[1], mask_shape[2]))
            masks = torch.concat((masks, padding))

        masks_out = torch.permute(torch.flatten(masks, start_dim=1, end_dim=2), (1, 0))

        sample = {'image': image, 'embedding': embed, 'mask' : masks_out}
        return sample

    def _load_embedding(self, id):
        path = self.coco.loadImgs(id)[0]["file_name"].split('.')[0] + '.npy'
        return torch.tensor(np.load(os.path.join(self.embed_path, path)))
    

