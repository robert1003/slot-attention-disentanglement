import os
import random
import json
import numpy as np
from PIL import Image
import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataloader import default_collate

class PARTNET(Dataset):
    def __init__(self, split='train'):
        super(PARTNET, self).__init__()

        assert split in ['train', 'val', 'test']
        self.split = split
        self.root_dir = your_path     
        self.files = os.listdir(self.root_dir)
        self.img_transform = transforms.Compose([
               transforms.ToTensor()])

    def __getitem__(self, index):
        path = self.files[index]
        image = Image.open(os.path.join(self.root_dir, path, "0.png")).convert("RGB")
        image = image.resize((128 , 128))
        image = self.img_transform(image)
        sample = {'image': image}

        return sample
            
    
    def __len__(self):
        return len(self.files)

class CLEVR(Dataset):
    def __init__(self, path, split='train', rescale=False):
        super(CLEVR, self).__init__()

        assert split in ['train', 'val', 'test']
        self.split = split
        self.root_dir = os.path.join(path, split)
        self.files = os.listdir(self.root_dir)
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])

    def __getitem__(self, index):
        path = self.files[index]
        image = Image.open(os.path.join(self.root_dir, path)).convert("RGB")
        image = image.resize((128 , 128))
        image = self.img_transform(image)
        sample = {'image': image}

        return sample

    def __len__(self):
        return len(self.files)
'''
Deprecated. This is for dataset randomly generated by genesis code
'''
class MultiDSprites(Dataset):
    def __init__(self, path='./data/multi_dsprites/processed', split='train', unique=True, num_slots=6, rescale=False):
        super(MultiDSprites, self).__init__()

        assert split in ['train', 'val', 'test']
        file_split = {'train': 'training', 'val': 'validation', 'test': 'test'}[split]
        self.split = split
        self.unique = unique
        self.data = np.load(os.path.join(path,
            file_split + '_images_rand4_' + ('unique' if unique else '') + '.npy'))
        self.mask = np.load(os.path.join(path,
            file_split + '_masks_rand4_' + ('unique' if unique else '') + '.npy'))
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])
        self.num_slots = num_slots

    def __getitem__(self, index):
        image = (self.data[index]*255).astype(np.uint8)
        mask = self.mask[index]
        # Resize mask
        mask = np.array(Image.fromarray(mask.squeeze(-1).astype(np.uint8), mode='L').resize((128,128), resample = Image.Resampling.NEAREST))
        # Convert to onehot
        onehot_masks = (np.arange(1,mask.max()+1) == mask[...,None]).astype(int)
        # pad zeros
        zeros_array = np.zeros((onehot_masks.shape[0],onehot_masks.shape[1], max(0,self.num_slots - onehot_masks.shape[2])))
        onehot_masks = np.concatenate((onehot_masks, zeros_array), axis=2)
        image = Image.fromarray(image)
        image = image.resize((128 , 128))
        image = self.img_transform(image)
        sample = {'image': image, 'mask': onehot_masks}

        return sample

    def __len__(self):
        return len(self.data)

'''
object num is 2-5
'''
class MultiDSpritesGrayBackground(Dataset):
    def __init__(self, path='./data/multi_dsprites/processed', rescale=False):
        super(MultiDSpritesGrayBackground, self).__init__()
        self.root_dir = path
        self.files = [i for i in os.listdir(self.root_dir) if 'image' in i]
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])

    def __getitem__(self, index):
        image_path = self.files[index]
        image = Image.fromarray(np.load(os.path.join(self.root_dir, image_path)))
        image = self.img_transform(image)

        mask_path = image_path.replace("image", "mask")
        mask = np.load(os.path.join(self.root_dir, mask_path))

        # Convert mask to 0-1 range
        mask = ((mask - mask.min()) / (mask.max() - mask.min())).astype(np.uint8)
        mask = torch.from_numpy(mask)

        # Remove background mask (always the first mask), background labels not included in ARI
        mask[0] = torch.zeros_like(mask[1])

        # Convert mask to format expected by ARI calc (H * W, # slots)
        mask = torch.permute(torch.flatten(mask.squeeze(-1), start_dim=1, end_dim=2), (1, 0))
        sample = {'image': image, 'mask': mask}
        return sample

    def __len__(self):
        return len(self.files)

'''
object num is 1-4
'''
class MultiDSpritesColorBackground(Dataset):
    def __init__(self, path='./data/multi_dsprites/processed', rescale=False):
        super(MultiDSpritesColorBackground, self).__init__()
        self.root_dir = path
        self.files = [i for i in os.listdir(self.root_dir) if 'image' in i]
        if not rescale:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor()])
        else:
            self.img_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(0.5, 0.5) # ([0,1]-0.5)/0.5=[-1,1]
                ])

    def __getitem__(self, index):
        image_path = self.files[index]
        image = Image.fromarray(np.load(os.path.join(self.root_dir, image_path)))
        image = self.img_transform(image)

        mask_path = image_path.replace("image", "mask")
        mask = np.load(os.path.join(self.root_dir, mask_path))

        # Convert mask to 0-1 range
        mask = ((mask - mask.min()) / (mask.max() - mask.min())).astype(np.uint8)
        mask = torch.from_numpy(mask)

        # Remove background mask (always the first mask), background labels not included in ARI
        mask[0] = torch.zeros_like(mask[1])

        # Convert mask to format expected by ARI calc (H * W, # slots)
        mask = torch.permute(torch.flatten(mask.squeeze(-1), start_dim=1, end_dim=2), (1, 0))
        sample = {'image': image, 'mask': mask}
        return sample

    def __len__(self):
        return len(self.files)
